# -*- coding: utf-8 -*-
"""Untitled26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15i20Czh4fDvJaqhgX_kVuhoi5e9FbKMN
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.utils import save_image, make_grid
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

# ==========================================
# PHASE 1: DATASET GENERATION (As requested)
# ==========================================
dataset_path = "Restoration_Dataset"
os.makedirs(f"{dataset_path}/corrupted", exist_ok=True)
os.makedirs(f"{dataset_path}/corrected", exist_ok=True)

print("--- Phase 1: Generating Dataset ---")
# Download MNIST
data = torchvision.datasets.MNIST(root='./temp_data', train=True, download=True)

# Generate pairs
print("Creating corrupted/corrected pairs...")
for i in range(1000):
    clean_img, _ = data[i]
    clean_img.save(f"{dataset_path}/corrected/img_{i}.png")

    img_tensor = transforms.ToTensor()(clean_img)
    noise = torch.randn_like(img_tensor) * 0.5
    noisy_tensor = torch.clip(img_tensor + noise, 0., 1.)

    noisy_img = transforms.ToPILImage()(noisy_tensor)
    noisy_img.save(f"{dataset_path}/corrupted/img_{i}.png")

print("Dataset Ready.\n")

# ==========================================
# PHASE 2: GAN FOR DENOISING (Modified Code)
# ==========================================
config = {
    'epochs': 20,
    'batch_size': 64,
    'learning_rate': 0.0002,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu'
}

# --- 1. Custom Dataset Loader ---
class PairedDataset(Dataset):
    def __init__(self, root_dir):
        self.root_dir = root_dir
        self.files = os.listdir(f"{root_dir}/corrected")
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (0.5,)) # Normalize to [-1, 1]
        ])

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        filename = self.files[idx]
        clean = Image.open(f"{self.root_dir}/corrected/{filename}")
        noisy = Image.open(f"{self.root_dir}/corrupted/{filename}")
        return self.transform(noisy), self.transform(clean)

dataloader = DataLoader(PairedDataset(dataset_path), batch_size=config['batch_size'], shuffle=True)

# --- 2. The Modified Generator (Cleaner) ---
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        # CHANGE 1: Input is now 28*28 (Image), not 100 (Noise)
        self.model = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(512),
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(1024),
            nn.Linear(1024, 28*28),
            nn.Tanh()
        )

    def forward(self, x):
        # Flatten image for Linear layer
        x = x.view(x.size(0), -1)
        img = self.model(x)
        img = img.view(img.size(0), 1, 28, 28)
        return img

# --- 3. The Discriminator (Judge) ---
# Kept exactly as in your code
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, img):
        img_flat = img.view(img.size(0), -1)
        validity = self.model(img_flat)
        return validity

# Initialize
generator = Generator().to(config['device'])
discriminator = Discriminator().to(config['device'])

optimizer_G = optim.Adam(generator.parameters(), lr=config['learning_rate'])
optimizer_D = optim.Adam(discriminator.parameters(), lr=config['learning_rate'])

adversarial_loss = nn.BCELoss()
pixel_loss = nn.MSELoss() # CHANGE 2: We need this to ensure the digit stays the same

# --- 4. Training Loop ---
print(f"--- Starting GAN Denoising Training ---")

for epoch in range(config['epochs']):
    for i, (noisy_imgs, clean_imgs) in enumerate(dataloader):

        # Move to GPU
        noisy_imgs = noisy_imgs.to(config['device'])
        clean_imgs = clean_imgs.to(config['device'])

        batch_size = noisy_imgs.size(0)
        valid = torch.ones(batch_size, 1).to(config['device'])
        fake = torch.zeros(batch_size, 1).to(config['device'])

        # -----------------
        #  Train Generator
        # -----------------
        optimizer_G.zero_grad()

        # Generate images (Input is NOISY IMAGE, not random noise)
        gen_imgs = generator(noisy_imgs)

        # Loss 1: Fool the discriminator (Make it look real/clean)
        g_adv_loss = adversarial_loss(discriminator(gen_imgs), valid)

        # Loss 2: Match the original image (Make it accurate)
        g_pixel_loss = pixel_loss(gen_imgs, clean_imgs)

        # Total Generator Loss (We weight pixel loss higher to force accuracy)
        g_loss = 0.001 * g_adv_loss + 0.999 * g_pixel_loss

        g_loss.backward()
        optimizer_G.step()

        # ---------------------
        #  Train Discriminator
        # ---------------------
        optimizer_D.zero_grad()

        real_loss = adversarial_loss(discriminator(clean_imgs), valid)
        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)
        d_loss = (real_loss + fake_loss) / 2

        d_loss.backward()
        optimizer_D.step()

    print(f"Epoch {epoch+1}/{config['epochs']} | D_Loss: {d_loss.item():.4f} | G_Loss: {g_loss.item():.4f}")

# ==========================================
# VISUALIZATION
# ==========================================
print("\n--- Visualizing Results ---")
# Get a batch
noisy_sample, clean_sample = next(iter(dataloader))
generator.eval()
with torch.no_grad():
    restored_sample = generator(noisy_sample.to(config['device'])).cpu()

# Plot
fig, axes = plt.subplots(3, 10, figsize=(20, 6))
for i in range(10):
    # Row 1: Corrupted
    axes[0, i].imshow(noisy_sample[i].squeeze(), cmap='gray')
    axes[0, i].axis('off')

    # Row 2: Restored (GAN Output)
    axes[1, i].imshow(restored_sample[i].squeeze(), cmap='gray')
    axes[1, i].axis('off')

    # Row 3: Original
    axes[2, i].imshow(clean_sample[i].squeeze(), cmap='gray')
    axes[2, i].axis('off')

axes[0,5].set_title("Input (Noisy)")
axes[1,5].set_title("GAN Output (Cleaned)")
axes[2,5].set_title("Target (Original)")
plt.show()